import Foundation
import AVFoundation
import PushToTalkCore

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º
public struct DialogueTranscription {
    public struct Turn {
        public let speaker: Speaker
        public let text: String

        public enum Speaker {
            case left   // –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 1)
            case right  // –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 2)

            var displayName: String {
                switch self {
                case .left: return "Speaker 1"
                case .right: return "Speaker 2"
                }
            }
        }
    }

    public let turns: [Turn]
    public let isStereo: Bool

    /// –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ –∫–∞–∫ —Ç–µ–∫—Å—Ç
    public func formatted() -> String {
        if !isStereo || turns.isEmpty {
            return turns.first?.text ?? ""
        }

        return turns.map { turn in
            "[\(turn.speaker.displayName)]: \(turn.text)"
        }.joined(separator: "\n\n")
    }
}

/// –°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ audio/video —Ñ–∞–π–ª–æ–≤
/// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç
public class FileTranscriptionService {
    private let whisperService: WhisperService

    public init(whisperService: WhisperService) {
        self.whisperService = whisperService
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç–µ—Ä–µ–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –î–∏–∞–ª–æ–≥ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º (–µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ)
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFileWithDialogue(at url: URL) async throws -> DialogueTranscription {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –¥–∏–∫—Ç–æ—Ä–æ–≤: \(url.lastPathComponent)")

        // 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–µ—Ä–µ–æ –ª–∏ —Ñ–∞–π–ª
        let channelCount = try await getChannelCount(from: url)
        LogManager.app.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: \(channelCount)")

        if channelCount == 2 {
            // –°—Ç–µ—Ä–µ–æ: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            return try await transcribeStereoAsDialogue(url: url)
        } else {
            // –ú–æ–Ω–æ: –æ–±—ã—á–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            let text = try await transcribeFile(at: url)
            return DialogueTranscription(
                turns: [DialogueTranscription.Turn(speaker: .left, text: text)],
                isStereo: false
            )
        }
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFile(at url: URL) async throws -> String {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞: \(url.lastPathComponent)")

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞
        let audioSamples = try await loadAudio(from: url)

        // 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∏—à–∏–Ω—É
        if SilenceDetector.shared.isSilence(audioSamples) {
            LogManager.app.info("üîá –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–∏—à–∏–Ω—É")
            throw FileTranscriptionError.silenceDetected
        }

        // 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
        let transcription = try await whisperService.transcribe(audioSamples: audioSamples)

        if transcription.isEmpty {
            throw FileTranscriptionError.emptyTranscription
        }

        LogManager.app.success("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(transcription.count) —Å–∏–º–≤–æ–ª–æ–≤")
        return transcription
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit (16kHz mono Float32)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞
    /// - Returns: –ú–∞—Å—Å–∏–≤ audio samples
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    private func loadAudio(from url: URL) async throws -> [Float] {
        let asset = AVAsset(url: url)

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç audio track
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            LogManager.app.failure("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç audio track", error: FileTranscriptionError.noAudioTrack)
            throw FileTranscriptionError.noAudioTrack
        }

        // –°–æ–∑–¥–∞–µ–º reader –¥–ª—è —á—Ç–µ–Ω–∏—è –∞—É–¥–∏–æ
        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, mono, Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            LogManager.app.failure("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞", error: FileTranscriptionError.readError)
            throw FileTranscriptionError.readError
        }

        var audioSamples: [Float] = []

        // –ß–∏—Ç–∞–µ–º –≤—Å–µ sample buffers
        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Data –≤ [Float]
                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                audioSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        let durationSeconds = Float(audioSamples.count) / 16000.0
        LogManager.app.success("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: \(audioSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (60 –º–∏–Ω—É—Ç = 3600s)
        let maxDuration: Float = 3600.0
        if durationSeconds > maxDuration {
            LogManager.app.warning("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (\(String(format: "%.0f", durationSeconds))s), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ \(String(format: "%.0f", maxDuration))s")
            let maxSamples = Int(maxDuration * 16000.0)
            return Array(audioSamples.prefix(maxSamples))
        }

        return audioSamples
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ
    private func getChannelCount(from url: URL) async throws -> Int {
        let asset = AVAsset(url: url)
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let formatDescriptions = try await audioTrack.load(.formatDescriptions)
        guard let formatDescription = formatDescriptions.first else {
            return 1 // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–æ–Ω–æ
        }

        if let audioStreamBasicDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription) {
            return Int(audioStreamBasicDescription.pointee.mChannelsPerFrame)
        }

        return 1
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å—Ç–µ—Ä–µ–æ —Ñ–∞–π–ª –∫–∞–∫ –¥–∏–∞–ª–æ–≥ (–ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ)
    private func transcribeStereoAsDialogue(url: URL) async throws -> DialogueTranscription {
        LogManager.app.info("üéß –°—Ç–µ—Ä–µ–æ —Ä–µ–∂–∏–º: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏–∫—Ç–æ—Ä–æ–≤")

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ
        let stereoSamples = try await loadAudioStereo(from: url)

        // 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã
        let leftChannel = extractChannel(from: stereoSamples, channel: 0)
        let rightChannel = extractChannel(from: stereoSamples, channel: 1)

        // 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª –æ—Ç–¥–µ–ª—å–Ω–æ
        LogManager.app.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –ª–µ–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 1)...")
        let leftText = try await whisperService.transcribe(audioSamples: leftChannel)

        LogManager.app.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 2)...")
        let rightText = try await whisperService.transcribe(audioSamples: rightChannel)

        // 4. –°–æ–∑–¥–∞—ë–º –¥–∏–∞–ª–æ–≥ —Å –ø–æ–ª–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ –æ—Ç –∫–∞–∂–¥–æ–≥–æ –¥–∏–∫—Ç–æ—Ä–∞
        var turns: [DialogueTranscription.Turn] = []

        // –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ä–µ–ø–ª–∏–∫–∏ –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (Speaker 1)
        if !leftText.isEmpty {
            turns.append(DialogueTranscription.Turn(speaker: .left, text: leftText))
        }

        // –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ä–µ–ø–ª–∏–∫–∏ –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (Speaker 2)
        if !rightText.isEmpty {
            turns.append(DialogueTranscription.Turn(speaker: .right, text: rightText))
        }

        LogManager.app.success("–°—Ç–µ—Ä–µ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ª–µ–≤—ã–π –∫–∞–Ω–∞–ª (\(leftText.count) —Å–∏–º–≤–æ–ª–æ–≤), –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (\(rightText.count) —Å–∏–º–≤–æ–ª–æ–≤)")

        return DialogueTranscription(turns: turns, isStereo: true)
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ (—Å–æ—Ö—Ä–∞–Ω—è—è –æ–±–∞ –∫–∞–Ω–∞–ª–∞)
    private func loadAudioStereo(from url: URL) async throws -> [[Float]] {
        let asset = AVAsset(url: url)

        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, STEREO (2 channels), Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 2,  // –°—Ç–µ—Ä–µ–æ!
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false  // Interleaved: L, R, L, R, ...
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            throw FileTranscriptionError.readError
        }

        var interleavedSamples: [Float] = []

        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                interleavedSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –º–∞—Å—Å–∏–≤ –∏–∑ –¥–≤—É—Ö –∫–∞–Ω–∞–ª–æ–≤ (–ø–æ–∫–∞ interleaved)
        return [interleavedSamples]
    }

    /// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –∏–∑ interleaved —Å—Ç–µ—Ä–µ–æ
    private func extractChannel(from stereoData: [[Float]], channel: Int) -> [Float] {
        guard let interleavedSamples = stereoData.first else { return [] }

        var channelSamples: [Float] = []
        channelSamples.reserveCapacity(interleavedSamples.count / 2)

        // Interleaved format: L, R, L, R, L, R, ...
        // channel 0 = left (indices 0, 2, 4, ...)
        // channel 1 = right (indices 1, 3, 5, ...)
        stride(from: channel, to: interleavedSamples.count, by: 2).forEach { index in
            channelSamples.append(interleavedSamples[index])
        }

        let durationSeconds = Float(channelSamples.count) / 16000.0
        LogManager.app.info("–ö–∞–Ω–∞–ª \(channel): \(channelSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        return channelSamples
    }

}

/// –û—à–∏–±–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤
enum FileTranscriptionError: LocalizedError {
    case noAudioTrack
    case readError
    case silenceDetected
    case emptyTranscription
    case fileTooLarge

    var errorDescription: String? {
        switch self {
        case .noAudioTrack:
            return "File does not contain an audio track"
        case .readError:
            return "Failed to read audio file"
        case .silenceDetected:
            return "File contains only silence"
        case .emptyTranscription:
            return "Transcription resulted in empty text"
        case .fileTooLarge:
            return "File is too large (max 60 minutes)"
        }
    }
}
