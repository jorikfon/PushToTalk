import Foundation
import AVFoundation
import PushToTalkCore

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
public struct DialogueTranscription {
    public struct Turn: Identifiable {
        public let id = UUID()  // –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è SwiftUI
        public let speaker: Speaker
        public let text: String
        public let startTime: TimeInterval  // –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ä–µ–ø–ª–∏–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        public let endTime: TimeInterval    // –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–µ–ø–ª–∏–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

        public enum Speaker {
            case left   // –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 1)
            case right  // –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 2)

            public var displayName: String {
                switch self {
                case .left: return "Speaker 1"
                case .right: return "Speaker 2"
                }
            }

            public var color: String {
                switch self {
                case .left: return "blue"
                case .right: return "orange"
                }
            }
        }

        public var duration: TimeInterval {
            return endTime - startTime
        }

        public init(speaker: Speaker, text: String, startTime: TimeInterval, endTime: TimeInterval) {
            self.speaker = speaker
            self.text = text
            self.startTime = startTime
            self.endTime = endTime
        }
    }

    public let turns: [Turn]
    public let isStereo: Bool
    public let totalDuration: TimeInterval  // –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏–∞–ª–æ–≥–∞

    public init(turns: [Turn], isStereo: Bool, totalDuration: TimeInterval = 0) {
        self.turns = turns
        self.isStereo = isStereo
        self.totalDuration = totalDuration
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ø–ª–∏–∫–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è timeline)
    public var sortedByTime: [Turn] {
        return turns.sorted { $0.startTime < $1.startTime }
    }

    /// –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ –∫–∞–∫ —Ç–µ–∫—Å—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
    public func formatted() -> String {
        if !isStereo || turns.isEmpty {
            return turns.first?.text ?? ""
        }

        return sortedByTime.map { turn in
            let timestamp = formatTimestamp(turn.startTime)
            return "[\(timestamp)] \(turn.speaker.displayName): \(turn.text)"
        }.joined(separator: "\n\n")
    }

    private func formatTimestamp(_ seconds: TimeInterval) -> String {
        let minutes = Int(seconds) / 60
        let secs = Int(seconds) % 60
        let millis = Int((seconds.truncatingRemainder(dividingBy: 1)) * 1000)
        return String(format: "%02d:%02d.%03d", minutes, secs, millis)
    }
}

/// –°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ audio/video —Ñ–∞–π–ª–æ–≤
/// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç
public class FileTranscriptionService {

    /// –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public enum TranscriptionMode {
        case vad        // –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Voice Activity Detection (–ª—É—á—à–µ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ)
        case batch      // –ü–∞–∫–µ—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ (–ª—É—á—à–µ –¥–ª—è –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
    }

    private let whisperService: WhisperService
    private var batchService: BatchTranscriptionService?

    /// –¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public var mode: TranscriptionMode = .batch  // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é batch –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ

    /// Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø—Ä–æ–≥—Ä–µ—Å—Å –∏ —Ä–µ–ø–ª–∏–∫–∏)
    public var onProgressUpdate: ((String, Double, DialogueTranscription?) -> Void)?

    public init(whisperService: WhisperService) {
        self.whisperService = whisperService
        self.batchService = BatchTranscriptionService(
            whisperService: whisperService,
            parameters: .lowQuality
        )
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç–µ—Ä–µ–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –î–∏–∞–ª–æ–≥ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º (–µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ)
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFileWithDialogue(at url: URL) async throws -> DialogueTranscription {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –¥–∏–∫—Ç–æ—Ä–æ–≤: \(url.lastPathComponent)")
        LogManager.app.info("–†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: \(self.mode == .batch ? "BATCH" : "VAD")")

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º batch —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω
        if mode == .batch {
            guard let batchService = batchService else {
                throw NSError(domain: "FileTranscriptionService", code: 1,
                            userInfo: [NSLocalizedDescriptionKey: "BatchTranscriptionService –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"])
            }

            // –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º callback –≤ batchService
            batchService.onProgressUpdate = onProgressUpdate

            return try await batchService.transcribe(url: url)
        }

        // VAD —Ä–µ–∂–∏–º (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
        // 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–µ—Ä–µ–æ –ª–∏ —Ñ–∞–π–ª
        let channelCount = try await getChannelCount(from: url)
        LogManager.app.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: \(channelCount)")

        if channelCount == 2 {
            // –°—Ç–µ—Ä–µ–æ: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            return try await transcribeStereoAsDialogue(url: url)
        } else {
            // –ú–æ–Ω–æ: –æ–±—ã—á–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            let audioSamples = try await loadAudio(from: url)
            let totalDuration = TimeInterval(audioSamples.count) / 16000.0
            let text = try await whisperService.transcribe(audioSamples: audioSamples)

            LogManager.app.info("–ú–æ–Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(text.count) —Å–∏–º–≤–æ–ª–æ–≤")

            let dialogue = DialogueTranscription(
                turns: [DialogueTranscription.Turn(
                    speaker: .left,
                    text: text,
                    startTime: 0,
                    endTime: totalDuration
                )],
                isStereo: false,
                totalDuration: totalDuration
            )

            // –í—ã–∑—ã–≤–∞–µ–º callback –¥–ª—è –º–æ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ç–æ–∂–µ
            onProgressUpdate?(url.lastPathComponent, 1.0, dialogue)

            return dialogue
        }
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFile(at url: URL) async throws -> String {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞: \(url.lastPathComponent)")

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞
        let audioSamples = try await loadAudio(from: url)

        // 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∏—à–∏–Ω—É
        if SilenceDetector.shared.isSilence(audioSamples) {
            LogManager.app.info("üîá –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–∏—à–∏–Ω—É")
            throw FileTranscriptionError.silenceDetected
        }

        // 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
        let transcription = try await whisperService.transcribe(audioSamples: audioSamples)

        if transcription.isEmpty {
            throw FileTranscriptionError.emptyTranscription
        }

        LogManager.app.success("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(transcription.count) —Å–∏–º–≤–æ–ª–æ–≤")
        return transcription
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit (16kHz mono Float32)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞
    /// - Returns: –ú–∞—Å—Å–∏–≤ audio samples
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    private func loadAudio(from url: URL) async throws -> [Float] {
        let asset = AVAsset(url: url)

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç audio track
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            LogManager.app.failure("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç audio track", error: FileTranscriptionError.noAudioTrack)
            throw FileTranscriptionError.noAudioTrack
        }

        // –°–æ–∑–¥–∞–µ–º reader –¥–ª—è —á—Ç–µ–Ω–∏—è –∞—É–¥–∏–æ
        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, mono, Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            LogManager.app.failure("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞", error: FileTranscriptionError.readError)
            throw FileTranscriptionError.readError
        }

        var audioSamples: [Float] = []

        // –ß–∏—Ç–∞–µ–º –≤—Å–µ sample buffers
        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Data –≤ [Float]
                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                audioSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        let durationSeconds = Float(audioSamples.count) / 16000.0
        LogManager.app.success("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: \(audioSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        // –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —É–±—Ä–∞–Ω–æ - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –ª—é–±–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        // –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ VAD

        return audioSamples
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ
    private func getChannelCount(from url: URL) async throws -> Int {
        let asset = AVAsset(url: url)
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let formatDescriptions = try await audioTrack.load(.formatDescriptions)
        guard let formatDescription = formatDescriptions.first else {
            return 1 // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–æ–Ω–æ
        }

        if let audioStreamBasicDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription) {
            return Int(audioStreamBasicDescription.pointee.mChannelsPerFrame)
        }

        return 1
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å—Ç–µ—Ä–µ–æ —Ñ–∞–π–ª –∫–∞–∫ –¥–∏–∞–ª–æ–≥ (–ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ)
    private func transcribeStereoAsDialogue(url: URL) async throws -> DialogueTranscription {
        LogManager.app.info("üéß –°—Ç–µ—Ä–µ–æ —Ä–µ–∂–∏–º: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏–∫—Ç–æ—Ä–æ–≤")

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ
        let stereoSamples = try await loadAudioStereo(from: url)

        // 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã
        let leftChannel = extractChannel(from: stereoSamples, channel: 0)
        let rightChannel = extractChannel(from: stereoSamples, channel: 1)

        // 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        let totalDuration = TimeInterval(leftChannel.count) / 16000.0

        // 4. –ò—Å–ø–æ–ª—å–∑—É–µ–º VAD –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –∫–∞–∂–¥–æ–º –∫–∞–Ω–∞–ª–µ
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º lowQuality –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ
        let vad = VoiceActivityDetector(parameters: .lowQuality)

        LogManager.app.info("üé§ VAD: –∞–Ω–∞–ª–∏–∑ –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞...")
        let leftSegments = vad.detectSpeechSegments(in: leftChannel)
        LogManager.app.info("–ù–∞–π–¥–µ–Ω–æ \(leftSegments.count) —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –ª–µ–≤–æ–º –∫–∞–Ω–∞–ª–µ")

        LogManager.app.info("üé§ VAD: –∞–Ω–∞–ª–∏–∑ –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞...")
        let rightSegments = vad.detectSpeechSegments(in: rightChannel)
        LogManager.app.info("–ù–∞–π–¥–µ–Ω–æ \(rightSegments.count) —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –ø—Ä–∞–≤–æ–º –∫–∞–Ω–∞–ª–µ")

        // 5. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
        var turns: [DialogueTranscription.Turn] = []
        let totalSegments = leftSegments.count + rightSegments.count
        var processedSegments = 0

        // –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –ª–µ–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 1)
        for segment in leftSegments {
            let segmentAudio = vad.extractAudio(for: segment, from: leftChannel)

            if !SilenceDetector.shared.isSilence(segmentAudio) {
                LogManager.app.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º Speaker 1: \(String(format: "%.1f", segment.startTime))s - \(String(format: "%.1f", segment.endTime))s")
                let text = try await whisperService.transcribe(audioSamples: segmentAudio)

                if !text.isEmpty {
                    turns.append(DialogueTranscription.Turn(
                        speaker: .left,
                        text: text,
                        startTime: segment.startTime,
                        endTime: segment.endTime
                    ))

                    // –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏
                    processedSegments += 1
                    let progress = Double(processedSegments) / Double(totalSegments)
                    let partialDialogue = DialogueTranscription(turns: turns, isStereo: true, totalDuration: totalDuration)
                    LogManager.app.debug("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: \(processedSegments)/\(totalSegments), turns: \(turns.count)")
                    onProgressUpdate?(url.lastPathComponent, progress, partialDialogue)
                } else {
                    LogManager.app.warning("Speaker 1: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ \(String(format: "%.1f", segment.startTime))s")
                }
            }
        }

        // –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 2)
        for segment in rightSegments {
            let segmentAudio = vad.extractAudio(for: segment, from: rightChannel)

            if !SilenceDetector.shared.isSilence(segmentAudio) {
                LogManager.app.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º Speaker 2: \(String(format: "%.1f", segment.startTime))s - \(String(format: "%.1f", segment.endTime))s")
                let text = try await whisperService.transcribe(audioSamples: segmentAudio)

                if !text.isEmpty {
                    turns.append(DialogueTranscription.Turn(
                        speaker: .right,
                        text: text,
                        startTime: segment.startTime,
                        endTime: segment.endTime
                    ))

                    // –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏
                    processedSegments += 1
                    let progress = Double(processedSegments) / Double(totalSegments)
                    let partialDialogue = DialogueTranscription(turns: turns, isStereo: true, totalDuration: totalDuration)
                    LogManager.app.debug("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: \(processedSegments)/\(totalSegments), turns: \(turns.count)")
                    onProgressUpdate?(url.lastPathComponent, progress, partialDialogue)
                } else {
                    LogManager.app.warning("Speaker 2: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ \(String(format: "%.1f", segment.startTime))s")
                }
            }
        }

        LogManager.app.success("–°—Ç–µ—Ä–µ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(turns.count) —Ä–µ–ø–ª–∏–∫ (\(leftSegments.count) –ª–µ–≤—ã—Ö, \(rightSegments.count) –ø—Ä–∞–≤—ã—Ö)")

        return DialogueTranscription(turns: turns, isStereo: true, totalDuration: totalDuration)
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ (—Å–æ—Ö—Ä–∞–Ω—è—è –æ–±–∞ –∫–∞–Ω–∞–ª–∞)
    private func loadAudioStereo(from url: URL) async throws -> [[Float]] {
        let asset = AVAsset(url: url)

        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, STEREO (2 channels), Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 2,  // –°—Ç–µ—Ä–µ–æ!
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false  // Interleaved: L, R, L, R, ...
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            throw FileTranscriptionError.readError
        }

        var interleavedSamples: [Float] = []

        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                interleavedSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –º–∞—Å—Å–∏–≤ –∏–∑ –¥–≤—É—Ö –∫–∞–Ω–∞–ª–æ–≤ (–ø–æ–∫–∞ interleaved)
        return [interleavedSamples]
    }

    /// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –∏–∑ interleaved —Å—Ç–µ—Ä–µ–æ
    private func extractChannel(from stereoData: [[Float]], channel: Int) -> [Float] {
        guard let interleavedSamples = stereoData.first else { return [] }

        var channelSamples: [Float] = []
        channelSamples.reserveCapacity(interleavedSamples.count / 2)

        // Interleaved format: L, R, L, R, L, R, ...
        // channel 0 = left (indices 0, 2, 4, ...)
        // channel 1 = right (indices 1, 3, 5, ...)
        stride(from: channel, to: interleavedSamples.count, by: 2).forEach { index in
            channelSamples.append(interleavedSamples[index])
        }

        let durationSeconds = Float(channelSamples.count) / 16000.0
        LogManager.app.info("–ö–∞–Ω–∞–ª \(channel): \(channelSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        return channelSamples
    }

}

/// –û—à–∏–±–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤
enum FileTranscriptionError: LocalizedError {
    case noAudioTrack
    case readError
    case silenceDetected
    case emptyTranscription
    case fileTooLarge

    var errorDescription: String? {
        switch self {
        case .noAudioTrack:
            return "File does not contain an audio track"
        case .readError:
            return "Failed to read audio file"
        case .silenceDetected:
            return "File contains only silence"
        case .emptyTranscription:
            return "Transcription resulted in empty text"
        case .fileTooLarge:
            return "File is too large (max 60 minutes)"
        }
    }
}
